import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col, when, lit
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DecimalType, TimestampType, LongType

# L·∫•y bi·∫øn m√¥i tr∆∞·ªùng (t·ª´ docker-compose)
KAFKA_SERVERS = os.getenv("KAFKA_SERVERS", "broker:29092")
CLICKHOUSE_URL = os.getenv("CLICKHOUSE_URL", "jdbc:clickhouse://clickhouse:8123/cdc_data?ssl=false")

# === 1. ƒê·ªäNH NGHƒ®A SCHEMA ===
# ƒê√¢y l√† c·∫•u tr√∫c JSON m√† Debezium publish l√™n Kafka.
# Ch√∫ng ta ch·ªâ quan t√¢m ƒë·∫øn 'payload' (d·ªØ li·ªáu) v√† 'op' (thao t√°c).

# Schema cho b·∫£ng 'customers'
CUSTOMERS_SCHEMA = StructType([
    StructField("id", IntegerType()),
    StructField("name", StringType()),
    StructField("email", StringType()),
    StructField("address", StringType()),
    StructField("created_at", TimestampType()) # Kh√¥ng d√πng nh∆∞ng ph·∫£i khai b√°o
])

# Schema cho b·∫£ng 'products'
PRODUCTS_SCHEMA = StructType([
    StructField("id", IntegerType()),
    StructField("name", StringType()),
    StructField("category", StringType()),
    StructField("price", DecimalType(10, 2)),
    StructField("created_at", TimestampType())
])

# Schema cho b·∫£ng 'orders'
ORDERS_SCHEMA = StructType([
    StructField("id", IntegerType()),
    StructField("customer_id", IntegerType()),
    StructField("order_date", TimestampType()),
    StructField("status", StringType()),
    StructField("total_amount", DecimalType(10, 2))
])

# Schema cho b·∫£ng 'order_items'
ORDER_ITEMS_SCHEMA = StructType([
    StructField("id", IntegerType()),
    StructField("order_id", IntegerType()),
    StructField("product_id", IntegerType()),
    StructField("quantity", IntegerType())
])

# Schema chung c·ªßa Debezium (ph·∫ßn ch√∫ng ta quan t√¢m)
DEBEZIUM_SCHEMA = StructType([
    StructField("before", StringType()), # D√πng cho delete
    StructField("after", StringType()),  # D√πng cho create/update
    StructField("op", StringType()),     # 'c' (create), 'u' (update), 'd' (delete)
    StructField("ts_ms", LongType())   # Timestamp (phi√™n b·∫£n)
])

# === 2. H√ÄM HELPER ƒê·ªÇ X·ª¨ L√ù STREAM ===
def process_stream(spark, topic_name, payload_schema, clickhouse_table):
    """
    H√†m n√†y ƒë·ªçc 1 topic Kafka, x·ª≠ l√Ω v√† ghi v√†o 1 b·∫£ng ClickHouse
    """
    
    # 1. ƒê·ªçc stream t·ª´ Kafka
    kafka_df = spark \
        .readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", KAFKA_SERVERS) \
        .option("subscribe", topic_name) \
        .option("startingOffsets", "earliest") \
        .load()

    # 2. Parse JSON c·ªßa Debezium
    # 'value' l√† JSON, ch√∫ng ta parse n√≥ d√πng DEBEZIUM_SCHEMA
    parsed_df = kafka_df \
        .select(from_json(col("value").cast("string"), DEBEZIUM_SCHEMA).alias("data")) \
        .select("data.*")

    # 3. X·ª≠ l√Ω logic CDC (Insert, Update, Delete)
    # N·∫øu 'op' = 'd' (delete), data n·∫±m trong 'before'.
    # N·∫øu 'op' = 'c' ho·∫∑c 'u' (create/update), data n·∫±m trong 'after'.
    # Ch√∫ng ta c≈©ng t·∫°o c·ªôt 'sign' (1 cho insert/update, -1 cho delete)
    
    # Parse JSON l·ªìng (payload_schema l√† schema c·ªßa b·∫£ng, vd: CUSTOMERS_SCHEMA)
    transformed_df = parsed_df \
        .withColumn("payload", 
            when(col("op") == "d", col("before")) # L·∫•y data c≈© n·∫øu l√† delete
            .otherwise(col("after"))               # L·∫•y data m·ªõi n·∫øu l√† create/update
        ) \
        .withColumn("payload", from_json(col("payload"), payload_schema)) \
        .withColumn("sign", 
            when(col("op") == "d", lit(-1)) # D·∫•u hi·ªáu delete
            .otherwise(lit(1))              # D·∫•u hi·ªáu insert/update
        ) \
        .select(
            "payload.*", # L·∫•y c√°c c·ªôt (id, name, email...)
            "ts_ms",     # C·ªôt phi√™n b·∫£n
            "sign"       # C·ªôt d·∫•u hi·ªáu
        )

    # 4. Ghi stream v√†o ClickHouse (d√πng forEachBatch)
    def write_to_clickhouse(batch_df, batch_id):
        print(f"Writing batch {batch_id} to ClickHouse table {clickhouse_table}...")
        try:
            batch_df.write \
                .format("jdbc") \
                .option("url", CLICKHOUSE_URL) \
                .option("dbtable", clickhouse_table) \
                .option("user", "admin") \
                .option("password", "admin") \
                .option("driver", "com.github.housepower.jdbc.ClickHouseDriver") \
                .option("batchsize", "5000") \
                .option("isolationLevel", "NONE") \
                .mode("append") \
                .save()
            print(f"Batch {batch_id} written successfully.")
        except Exception as e:
            print(f"‚ùå Error writing batch {batch_id}: {e}")

    # 5. B·∫Øt ƒë·∫ßu query
    query = transformed_df \
        .writeStream \
        .foreachBatch(write_to_clickhouse) \
        .option("checkpointLocation", f"/tmp/checkpoints/{topic_name}") \
        .start()
        
    return query

# === 3. KH·ªûI T·∫†O SPARK V√Ä CH·∫†Y C√ÅC STREAM ===
def main():
    spark = SparkSession.builder \
        .appName("CDC_Kafka_to_ClickHouse") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN") # Gi·∫£m log nhi·ªÖu
    
    print("üöÄ Spark Session created. Starting streams...")

    # Ch·∫°y 4 stream song song cho 4 b·∫£ng
    query_customers = process_stream(spark, "cdc.public.customers", CUSTOMERS_SCHEMA, "cdc_data.customers")
    query_products = process_stream(spark, "cdc.public.products", PRODUCTS_SCHEMA, "cdc_data.products")
    query_orders = process_stream(spark, "cdc.public.orders", ORDERS_SCHEMA, "cdc_data.orders")
    query_order_items = process_stream(spark, "cdc.public.order_items", ORDER_ITEMS_SCHEMA, "cdc_data.order_items")

    # Ch·ªù t·∫•t c·∫£ c√°c stream... (n·∫øu 1 c√°i s·∫≠p, t·∫•t c·∫£ s·∫Ω d·ª´ng)
    spark.streams.awaitAnyTermination()

if __name__ == "__main__":
    main()